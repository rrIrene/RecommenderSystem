{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(13) #TODO Check if this is used for sgd\n",
    "import keras.backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Reshape, Lambda\n",
    "from keras.utils import np_utils\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.preprocessing import sequence\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.neighbors import NearestNeighbors as nn\n",
    "from matplotlib import pylab\n",
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT Modify the lines in this cell\n",
    "path = 'alice.txt'\n",
    "corpus = open(path).readlines()[0:700]\n",
    "\n",
    "corpus = [sentence for sentence in corpus if sentence.count(\" \") >= 2]\n",
    "\n",
    "tokenizer = Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'+\"'\")\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "corpus = tokenizer.texts_to_sequences(corpus)\n",
    "nb_samples = sum(len(s) for s in corpus)\n",
    "V = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Is this something they need to change?\n",
    "dim = 100\n",
    "window_size = 2 #use this window size for Skipgram, CBOW, and the model with the additional hidden layer\n",
    "window_size_corpus = 4 #use this window size for the co-occurrence matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "### Co-occurrence Matrix\n",
    "Use the provided code to load the \"Alice in Wonderland\" text document. \n",
    "1. Implement the word-word co-occurrence matrix for “Alice in Wonderland”\n",
    "2. Normalize the words such that every value lies within a range of 0 and 1\n",
    "3. Compute the cosine distance between the given words:\n",
    "    - Alice \n",
    "    - Dinah\n",
    "    - Rabbit\n",
    "4. List the 5 closest words to 'Alice'. Discuss the results.\n",
    "5. Discuss what the main drawbacks are of a term-term co-occurence matrix solutions?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>happened</th>\n",
       "      <th>seemed</th>\n",
       "      <th>way</th>\n",
       "      <th>indeed</th>\n",
       "      <th>learnt</th>\n",
       "      <th>overhead</th>\n",
       "      <th>ring</th>\n",
       "      <th>salt</th>\n",
       "      <th>not</th>\n",
       "      <th>chin</th>\n",
       "      <th>...</th>\n",
       "      <th>laugh</th>\n",
       "      <th>splashing</th>\n",
       "      <th>cake</th>\n",
       "      <th>slowly</th>\n",
       "      <th>felt</th>\n",
       "      <th>take</th>\n",
       "      <th>pour</th>\n",
       "      <th>wondering</th>\n",
       "      <th>family</th>\n",
       "      <th>understand</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>happened</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.208514</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>seemed</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>way</th>\n",
       "      <td>0.039873</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>indeed</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>learnt</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1182 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          happened  seemed       way  indeed  learnt  overhead  ring  salt  \\\n",
       "happened  0.000000     0.0  0.208514     0.0     0.0       0.0   0.0   0.0   \n",
       "seemed    0.000000     0.0  0.000000     0.0     0.0       0.0   0.0   0.0   \n",
       "way       0.039873     0.0  0.000000     0.0     0.0       0.0   0.0   0.0   \n",
       "indeed    0.000000     0.0  0.000000     0.0     0.0       0.0   0.0   0.0   \n",
       "learnt    0.000000     0.0  0.000000     0.0     0.0       0.0   0.0   0.0   \n",
       "\n",
       "          not  chin     ...      laugh  splashing  cake  slowly  felt  take  \\\n",
       "happened  0.0   0.0     ...        0.0        0.0   0.0     0.0   0.0   0.0   \n",
       "seemed    0.0   0.0     ...        0.0        0.0   0.0     0.0   0.0   0.0   \n",
       "way       0.0   0.0     ...        0.0        0.0   0.0     0.0   0.0   0.0   \n",
       "indeed    0.0   0.0     ...        0.0        0.0   0.0     0.0   0.0   0.0   \n",
       "learnt    0.0   0.0     ...        0.0        0.0   0.0     0.0   0.0   0.0   \n",
       "\n",
       "          pour  wondering  family  understand  \n",
       "happened   0.0        0.0     0.0         0.0  \n",
       "seemed     0.0        0.0     0.0         0.0  \n",
       "way        0.0        0.0     0.0         0.0  \n",
       "indeed     0.0        0.0     0.0         0.0  \n",
       "learnt     0.0        0.0     0.0         0.0  \n",
       "\n",
       "[5 rows x 1182 columns]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create co-occurrence matrix\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "\n",
    "#dataset with unique words as index\n",
    "words = list(tokenizer.word_index.keys())\n",
    "matrix = pd.DataFrame(words, columns=['words'])\n",
    "matrix.set_index('words', inplace=True)\n",
    "columns = pd.DataFrame(columns=words)\n",
    "matrix = pd.concat([matrix,columns])\n",
    "matrix = matrix.reindex(columns.columns, axis=1)\n",
    "matrix.fillna(0, inplace=True)\n",
    "\n",
    "#inverse index to get word by code\n",
    "inverse_index = dict((v,k) for k, v in tokenizer.word_index.items())\n",
    "\n",
    "#compute score for every word-word couple\n",
    "for line in corpus:\n",
    "    for i, word_code in enumerate(line):\n",
    "        word = inverse_index.get(word_code)\n",
    "        for j in range(max(0, i-window_size_corpus), min(len(line), i+window_size_corpus+1)):\n",
    "            if word_code != line[j]:\n",
    "                matrix[word][inverse_index.get(line[j])] += 1\n",
    "\n",
    "#normalize each sample\n",
    "normalized_values = preprocessing.normalize(matrix.values)\n",
    "matrix = pd.DataFrame(normalized_values, index=matrix.index, columns=matrix.columns)\n",
    "\n",
    "matrix.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between Alice and Rabbit: 0.054617805224162\n",
      "Cosine similarity between Alice and Dinah: 0.05073318700003422\n",
      "Cosine similarity between Dinah and Rabbit: 0.034243448025346485\n"
     ]
    }
   ],
   "source": [
    "#find cosine similarity to Alice, Dinah and Rabbit\n",
    "\n",
    "alice_vector = matrix['alice'].values.reshape(1, -1)\n",
    "rabbit_vector = matrix['rabbit'].values.reshape(1, -1)\n",
    "dinah_vector = matrix['dinah'].values.reshape(1, -1)\n",
    "\n",
    "alice_vs_rabbit = cosine_similarity(alice_vector, rabbit_vector)\n",
    "alice_vs_dinah = cosine_similarity(alice_vector, dinah_vector)\n",
    "dinah_vs_rabbit = cosine_similarity(dinah_vector, rabbit_vector)\n",
    "\n",
    "print(\"Cosine similarity between Alice and Rabbit: \" + str(alice_vs_rabbit[0][0]))\n",
    "print(\"Cosine similarity between Alice and Dinah: \" + str(alice_vs_dinah[0][0]))\n",
    "print(\"Cosine similarity between Dinah and Rabbit: \" + str(dinah_vs_rabbit[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: i\n",
      "Distance: 3.6581715798218193\n",
      "\n",
      "Word: she\n",
      "Distance: 3.662735140275419\n",
      "\n",
      "Word: a\n",
      "Distance: 3.664105104961175\n",
      "\n",
      "Word: very\n",
      "Distance: 3.665855124516278\n",
      "\n",
      "Word: it\n",
      "Distance: 3.66876538790552\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#find the closest words to Alice (nearest neighbors)\n",
    "\n",
    "neigh = nn()\n",
    "neigh.fit(matrix.values)\n",
    "\n",
    "neighbors = neigh.kneighbors(alice_vector, 6)\n",
    "\n",
    "for i, n in enumerate(neighbors[1][0]):\n",
    "    if matrix.index[n] != 'alice':\n",
    "        print(\"Word: {}\\nDistance: {}\\n\".format(matrix.index[n], neighbors[0][0][i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The five closest words to \"Alice\" appear to be: \"I\", \"she\", \"a\", \"very\" and \"it\". It makes sense that the personal pronouns \"she\" and \"i\" are the closest ones to \"Alice\", since they are used in very similiar contexts and they are probably often followed by the same verbs. E.g. \"Alice sees the rabbit\", \"she sees the rabbit\" or \"I see the rabbit\", in direct dialogs. As for the other three words, they do not bring valuable information, as they are all quite common in the english language. A possible way to avoid this would be to remove stopwords from the corpus, in order to focus only on more meaningful terms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussion of the drawbacks:\n",
    "- **Sparse matrix:** As can be seen in the output of the matrix, the word pair 'particular' and 'suddenly' never appear close to each other in the corpus, but there is still a cell reserved in the matrix for that pair. The same goes for many other pairs in the matrix, resulting in many cells with value 0. Obviously, this is not very efficient for storage.\n",
    "- **Large matrix:** As can be seen in the word co-occurrence matrix for Alice In Wonderland that was created above, the matrix size is 1182 rows x 1182 columns, where 1182 is the number of tokens in the corpus. It is easy to imagine that this will become a problem for a bigger corpus, or for a corpus which has dynamic content, e.g. a web search engine.\n",
    "- **Non-discriminative results:** As shown in the nearest neighbors calculation above, the words 'a', 'it', and 'very' do not give very meaningful correlation. However, they are among the most common words used in English language, so it makes sense that they appear often near the term 'Alice' (or any other terms in the corpus). Therefore, stopwords filter or even a different weighting score other than word occurence count should be used in order to find words correlation that is more meaningful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save your all the vector representations of your word embeddings in this way\n",
    "#Change when necessary the sizes of the vocabulary/embedding dimension\n",
    "\n",
    "f = open('vectors_co_occurrence.txt',\"w\")\n",
    "f.write(\" \".join([str(V-1),str(V-1)]))\n",
    "f.write(\"\\n\")\n",
    "\n",
    "#vectors = your word co-occurrence matrix\n",
    "vectors = matrix.values\n",
    "for i, word in enumerate(tokenizer.word_index.keys()): \n",
    "    f.write(word)\n",
    "    f.write(\" \")\n",
    "    f.write(\" \".join(map(str, list(vectors[i,:]))))\n",
    "    f.write(\"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reopen your file as follows\n",
    "\n",
    "co_occurrence = KeyedVectors.load_word2vec_format('./vectors_co_occurrence.txt', binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "### Word embeddings\n",
    "Build embeddings with a keras implementation where the embedding vector is of length 50, 150 and 300. Use the Alice in Wonderland text book for training.\n",
    "1. Using the CBOW model\n",
    "2. Using Skipgram model\n",
    "3. Add extra hidden dense layer to CBow and Skipgram implementations. Choose an activation function for that layer and justify your answer.\n",
    "4. Analyze the four different word embeddings\n",
    "    - Implement your own function to perform the analogy task with. Do not use existing libraries for this task such as Gensim. Your function should be able to answer whether an anaology as in the example given in the pdf-file is true.\n",
    "    - Compare the performance on the analogy task between the word embeddings that you have trained in 2.1, 2.2 and 2.3.  \n",
    "    - Visualize your results and interpret your results\n",
    "5. Use the word co-occurence matrix from Question 1. Compare the performance on the analogy task with the performance of your trained word embeddings.  \n",
    "6. Discuss:\n",
    "    - What are the main advantages of CBOW and Skipgram?\n",
    "    - What is the advantage of negative sampling?\n",
    "    - What are the main drawbacks of CBOW and Skipgram?\n",
    "7. Load pre-trained embeddings on large corpuses (see the pdf file). You only have to consider the word embeddings with an embedding size of 300\n",
    "    - Compare performance on the analogy task with your own trained embeddings from \"Alice in Wonderland\". You can limit yourself to the vocabulary of Alice in Wonderland. Visualize the pre-trained word embeddings and compare these with the results of your own trained word embeddings. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function definitions for CBOW\n",
    "\n",
    "#generate data for CBOW\n",
    "def generate_data_cbow(corpus, window_size, V):\n",
    "    maxlen = window_size*2\n",
    "    all_in = []\n",
    "    all_out = []\n",
    "    for words in corpus:\n",
    "        L = len(words)\n",
    "        for index, word in enumerate(words):\n",
    "            contexts = []\n",
    "            labels   = []            \n",
    "            s = index - window_size\n",
    "            e = index + window_size + 1\n",
    "            \n",
    "            contexts.append([words[i] for i in range(s, e) if 0 <= i < L and i != index])\n",
    "            labels.append(word)\n",
    "            \n",
    "            all_in.append(sequence.pad_sequences(contexts, maxlen=maxlen))\n",
    "            all_out.append(np_utils.to_categorical(labels, V))\n",
    "    return (all_in,all_out)\n",
    "\n",
    "#load the preprocessed CBOW data\n",
    "def generate_data_cbow_from_file():\n",
    "    f = open('data_cbow.txt' ,'r')\n",
    "    for row in f:\n",
    "        inputs,outputs = row.split(\",\")\n",
    "        inputs = np.fromstring(inputs, dtype=int, sep=' ').reshape(1,-1)\n",
    "        outputs = np.fromstring(outputs, dtype=float, sep=' ').reshape(1,-1)\n",
    "        yield (inputs,outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#prepare data for cbow\n",
    "   \n",
    "#get x and y's for data\n",
    "x,y = generate_data_cbow(corpus,window_size,V)\n",
    "\n",
    "#save the preprocessed data of CBOW\n",
    "f = open('data_cbow.txt' ,'w')\n",
    "\n",
    "for input,outcome  in zip(x,y):\n",
    "    input = np.concatenate(input)\n",
    "    f.write(\" \".join(map(str, list(input))))\n",
    "    f.write(\",\")\n",
    "    outcome = np.concatenate(outcome)\n",
    "    f.write(\" \".join(map(str,list(outcome))))\n",
    "    f.write(\"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training cbow for dim=50\n",
      "0 41687.05540704727\n",
      "1 39124.244715213776\n",
      "2 38936.51728707552\n",
      "3 38850.57306432724\n",
      "4 38804.08415496349\n",
      "5 38763.57433068752\n",
      "6 38715.587130606174\n",
      "7 38671.027919739485\n",
      "8 38635.33781383932\n",
      "9 38615.43659273535\n",
      "\n",
      "Training cbow for dim=150\n",
      "0 41629.50603675842\n",
      "1 38776.44107758999\n",
      "2 38392.79202771187\n",
      "3 38186.12114664912\n",
      "4 37977.94337449968\n",
      "5 37776.427670001984\n",
      "6 37599.132757760584\n",
      "7 37463.50423672795\n",
      "8 37357.93073264323\n",
      "9 37265.268496920355\n",
      "\n",
      "Training cbow for dim=300\n",
      "0 41567.44344806671\n",
      "1 38521.191526293755\n",
      "2 37995.913539111614\n",
      "3 37700.00315974653\n",
      "4 37419.71258917451\n",
      "5 37160.13435649127\n",
      "6 36951.031872563064\n",
      "7 36790.47656428348\n",
      "8 36659.9330386878\n",
      "9 36543.056479451014\n"
     ]
    }
   ],
   "source": [
    "#create CBOW model\n",
    "\n",
    "for dim in [50, 150, 300]:\n",
    "    #create model\n",
    "    cbow = Sequential()\n",
    "    cbow.add(Embedding(input_dim=V, output_dim=dim, input_length=window_size*2))\n",
    "    cbow.add(Lambda(lambda x: K.mean(x, axis=1), output_shape=(dim,)))\n",
    "    cbow.add(Dense(input_dim=dim, units=V, kernel_initializer='uniform', activation='softmax'))\n",
    "    \n",
    "    #define loss function for CBOW\n",
    "    cbow.compile(loss='categorical_crossentropy', optimizer='adadelta')\n",
    "    \n",
    "    #train CBOW model\n",
    "    print(\"\\nTraining cbow for dim=\"+str(dim))\n",
    "    for ite in range(10):\n",
    "        loss = 0.\n",
    "        for x, y in generate_data_cbow_from_file():\n",
    "            loss += cbow.train_on_batch(x, y)\n",
    "        print(ite, loss)\n",
    "    \n",
    "    #save vector representation to file\n",
    "    f = open('vectors_cbow_'+str(dim)+'.txt' ,'w')\n",
    "    f.write(\" \".join([str(V-1),str(dim)]))\n",
    "    f.write(\"\\n\")\n",
    "\n",
    "    vectors = cbow.get_weights()[0]\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        f.write(word)\n",
    "        f.write(\" \")\n",
    "        f.write(\" \".join(map(str, list(vectors[i,:]))))\n",
    "        f.write(\"\\n\")\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define loss function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function definitions for Skipgram\n",
    "\n",
    "#generate data for Skipgram\n",
    "def generate_data_skipgram(corpus, window_size, V):\n",
    "    maxlen = window_size*2\n",
    "    all_in = []\n",
    "    all_out = []\n",
    "    for words in corpus:\n",
    "        L = len(words)\n",
    "        for index, word in enumerate(words):\n",
    "            p = index - window_size\n",
    "            n = index + window_size + 1\n",
    "                    \n",
    "            in_words = []\n",
    "            labels = []\n",
    "            for i in range(p, n):\n",
    "                if i != index and 0 <= i < L:\n",
    "                    in_words.append([word])\n",
    "                    labels.append(words[i])\n",
    "            if in_words != []:\n",
    "                all_in.append(np.array(in_words,dtype=np.int32))\n",
    "                all_out.append(np_utils.to_categorical(labels, V))\n",
    "    return (all_in,all_out)\n",
    "\n",
    "#load the preprocessed Skipgram data\n",
    "def generate_data_skipgram_from_file():\n",
    "    f = open('data_skipgram.txt' ,'r')\n",
    "    for row in f:\n",
    "        inputs,outputs = row.split(\",\")\n",
    "        inputs = np.fromstring(inputs, dtype=int, sep=' ')\n",
    "        inputs = np.asarray(np.split(inputs, len(inputs)))\n",
    "        outputs = np.fromstring(outputs, dtype=float, sep=' ')\n",
    "        outputs = np.asarray(np.split(outputs, len(inputs)))\n",
    "        yield (inputs,outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare data for Skipgram\n",
    "   \n",
    "#get x and y's for data\n",
    "x,y = generate_data_skipgram(corpus,window_size,V)\n",
    "\n",
    "#save the preprocessed data of Skipgram\n",
    "f = open('data_skipgram.txt' ,'w')\n",
    "\n",
    "for input,outcome  in zip(x,y):\n",
    "    input = np.concatenate(input)\n",
    "    f.write(\" \".join(map(str, list(input))))\n",
    "    f.write(\",\")\n",
    "    outcome = np.concatenate(outcome)\n",
    "    f.write(\" \".join(map(str,list(outcome))))\n",
    "    f.write(\"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training skipgram for dim=50\n",
      "0 41271.83753204346\n",
      "1 39101.409532785416\n",
      "2 39265.025777578354\n",
      "3 39357.28577589989\n",
      "4 39443.99534595013\n",
      "5 39536.58625638485\n",
      "6 39641.83393859863\n",
      "7 39760.61089479923\n",
      "8 39890.62479567528\n",
      "9 40026.53038358688\n",
      "\n",
      "Training skipgram for dim=150\n",
      "0 41215.42183089256\n",
      "1 38922.641077041626\n",
      "2 38984.99912106991\n",
      "3 39005.49183559418\n",
      "4 39030.080889582634\n",
      "5 39068.81769824028\n",
      "6 39119.45306658745\n",
      "7 39171.69657659531\n",
      "8 39215.19959139824\n",
      "9 39248.661136984825\n",
      "\n",
      "Training skipgram for dim=300\n",
      "0 41158.671072006226\n",
      "1 38737.375608444214\n",
      "2 38703.30918824673\n",
      "3 38658.181151390076\n",
      "4 38628.1092761755\n",
      "5 38608.344398379326\n",
      "6 38582.745631456375\n",
      "7 38541.39749741554\n",
      "8 38489.80800497532\n",
      "9 38436.15457010269\n"
     ]
    }
   ],
   "source": [
    "#create Skipgram model\n",
    "\n",
    "for dim in [50, 150, 300]:\n",
    "    #create model\n",
    "    skipgram = Sequential()\n",
    "    skipgram.add(Embedding(input_dim=V, output_dim=dim, embeddings_initializer='glorot_uniform', input_length=1))\n",
    "    skipgram.add(Reshape((dim, )))\n",
    "    skipgram.add(Dense(input_dim=dim, units=V, kernel_initializer='uniform', activation='softmax'))\n",
    "    \n",
    "    #define loss function for Skipgram\n",
    "    skipgram.compile(loss='categorical_crossentropy', optimizer='adadelta')\n",
    "    \n",
    "    #train skipgram model\n",
    "    print(\"\\nTraining skipgram for dim=\"+str(dim))\n",
    "    for ite in range(10):\n",
    "        loss = 0.\n",
    "        for x, y in generate_data_skipgram_from_file():  \n",
    "            loss += skipgram.train_on_batch(x, y)\n",
    "        print(ite, loss)\n",
    "    \n",
    "    #save vector representation to file\n",
    "    f = open('vectors_skipgram_'+str(dim)+'.txt' ,'w')\n",
    "    f.write(\" \".join([str(V-1),str(dim)]))\n",
    "    f.write(\"\\n\")\n",
    "\n",
    "    vectors = skipgram.get_weights()[0]\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        f.write(word)\n",
    "        f.write(\" \")\n",
    "        f.write(\" \".join(map(str, list(vectors[i,:]))))\n",
    "        f.write(\"\\n\")\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training cbow for dim=50\n",
      "0 39876.976503133774\n",
      "1 38887.68875396252\n",
      "2 38680.05655413866\n",
      "3 38352.927188932896\n",
      "4 37978.264512464404\n",
      "5 37646.310855060816\n",
      "6 37400.60246942192\n",
      "7 37169.78207338974\n",
      "8 37007.83139162883\n",
      "9 36828.35906231403\n",
      "\n",
      "Training cbow for dim=150\n",
      "0 39654.35764867067\n",
      "1 38465.63978609443\n",
      "2 37786.18354636431\n",
      "3 37208.35590395704\n",
      "4 36812.87390038185\n",
      "5 36532.75554645341\n",
      "6 36401.771414480405\n",
      "7 36449.23277854663\n",
      "8 35718.332754083094\n",
      "9 35346.63373138767\n",
      "\n",
      "Training cbow for dim=300\n",
      "0 39637.41914212704\n",
      "1 38149.99522295594\n",
      "2 37399.316540531814\n",
      "3 36763.60208453052\n",
      "4 36193.30322461575\n",
      "5 35597.70400031423\n",
      "6 35028.675611557905\n",
      "7 34477.128915025154\n",
      "8 33701.826102928666\n",
      "9 33224.047257302154\n"
     ]
    }
   ],
   "source": [
    "#create CBOW model with additional dense layer\n",
    "\n",
    "for dim in [50, 150, 300]:\n",
    "    #create model\n",
    "    cbow = Sequential()\n",
    "    cbow.add(Embedding(input_dim=V, output_dim=dim, input_length=window_size*2))\n",
    "    cbow.add(Lambda(lambda x: K.mean(x, axis=1), output_shape=(dim,)))\n",
    "    cbow.add(Dense(256, activation=\"relu\"))\n",
    "    cbow.add(Dense(units=V, kernel_initializer='uniform', activation='softmax'))\n",
    "    \n",
    "    #define loss function for CBOW + dense\n",
    "    cbow.compile(loss='categorical_crossentropy', optimizer='adadelta')\n",
    "    \n",
    "    #train CBOW + dense model\n",
    "    print(\"\\nTraining cbow with dense layer for dim=\"+str(dim))\n",
    "    for ite in range(10):\n",
    "        loss = 0.\n",
    "        for x, y in generate_data_cbow_from_file():\n",
    "            loss += cbow.train_on_batch(x, y)\n",
    "        print(ite, loss)\n",
    "    \n",
    "    #save vector representation to file\n",
    "    f = open('vectors_cbow_dense_'+str(dim)+'.txt' ,'w')\n",
    "    f.write(\" \".join([str(V-1),str(dim)]))\n",
    "    f.write(\"\\n\")\n",
    "\n",
    "    vectors = cbow.get_weights()[0]\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        f.write(word)\n",
    "        f.write(\" \")\n",
    "        f.write(\" \".join(map(str, list(vectors[i,:]))))\n",
    "        f.write(\"\\n\")\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training skipgram with dense layer for dim=50\n",
      "0 39207.16420960426\n",
      "1 38717.52406477928\n",
      "2 38526.44444894791\n",
      "3 38412.45753622055\n",
      "4 38297.32982969284\n",
      "5 38201.73449134827\n",
      "6 38118.846017599106\n",
      "7 38042.04780900478\n",
      "8 37966.90042972565\n",
      "9 37903.70432114601\n",
      "\n",
      "Training skipgram with dense layer for dim=150\n",
      "0 39099.298688173294\n",
      "1 38404.9980853796\n",
      "2 38174.07791543007\n",
      "3 37972.19873189926\n",
      "4 37827.99967753887\n",
      "5 37719.63192629814\n",
      "6 37627.54604494572\n",
      "7 37583.506836652756\n",
      "8 37550.056842803955\n",
      "9 37528.101528167725\n",
      "\n",
      "Training skipgram with dense layer for dim=300\n",
      "0 39111.37614989281\n",
      "1 38343.57023358345\n",
      "2 38188.41474318504\n",
      "3 38083.84280073643\n",
      "4 38030.5060955286\n",
      "5 38014.14457964897\n",
      "6 38032.269528090954\n",
      "7 38007.4168151021\n",
      "8 37993.029007434845\n",
      "9 37960.496544241905\n"
     ]
    }
   ],
   "source": [
    "#create Skipgram model with additional dense layer\n",
    "\n",
    "for dim in [50, 150, 300]:\n",
    "    #create model\n",
    "    skipgram = Sequential()\n",
    "    skipgram.add(Embedding(input_dim=V, output_dim=dim, embeddings_initializer='glorot_uniform', input_length=1))\n",
    "    skipgram.add(Reshape((dim, )))\n",
    "    skipgram.add(Dense(256, activation=\"relu\"))\n",
    "    skipgram.add(Dense(units=V, kernel_initializer='uniform', activation='softmax'))\n",
    "    \n",
    "    #define loss function for Skipgram + dense\n",
    "    skipgram.compile(loss='categorical_crossentropy', optimizer='adadelta')\n",
    "    \n",
    "    #train model for Skipgram + dense\n",
    "    print(\"\\nTraining skipgram with dense layer for dim=\"+str(dim))\n",
    "    for ite in range(10):\n",
    "        loss = 0.\n",
    "        for x, y in generate_data_skipgram_from_file():\n",
    "            loss += skipgram.train_on_batch(x, y)\n",
    "        print(ite, loss)\n",
    "    \n",
    "    #save vector representation to file\n",
    "    f = open('vectors_skipgram_dense_'+str(dim)+'.txt' ,'w')\n",
    "    f.write(\" \".join([str(V-1),str(dim)]))\n",
    "    f.write(\"\\n\")\n",
    "\n",
    "    vectors = skipgram.get_weights()[0]\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        f.write(word)\n",
    "        f.write(\" \")\n",
    "        f.write(\" \".join(map(str, list(vectors[i,:]))))\n",
    "        f.write(\"\\n\")\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>Activation function</strong><br>\n",
    "In order to add non-linearity to the models, we choosed ReLU (Rectified Linear Unit) as the activation function for the hidden Dense layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load vectors from files\n",
    "cbows = []\n",
    "cbows_dense = []\n",
    "skipgrams = []\n",
    "skipgrams_dense = []\n",
    "    \n",
    "for dim in [50, 150, 300]:\n",
    "    f = open('vectors_skipgram_'+str(dim)+'.txt' ,'r')\n",
    "    d = {}\n",
    "    for row in f.readlines()[1:]:\n",
    "        line = row.split()\n",
    "        d[line[0]] = line[1:]\n",
    "    skipgrams.append(d)\n",
    "    \n",
    "    f = open('vectors_skipgram_dense_'+str(dim)+'.txt' ,'r')\n",
    "    d = {}\n",
    "    for row in f.readlines()[1:]:\n",
    "        line = row.split()\n",
    "        d[line[0]] = line[1:]\n",
    "    skipgrams_dense.append(d)\n",
    "    \n",
    "    f = open('vectors_cbow_'+str(dim)+'.txt' ,'r')\n",
    "    d = {}\n",
    "    for row in f.readlines()[1:]:\n",
    "        line = row.split()\n",
    "        d[line[0]] = line[1:]\n",
    "    cbows.append(d)\n",
    "    \n",
    "    f = open('vectors_cbow_dense_'+str(dim)+'.txt' ,'r')\n",
    "    d = {}\n",
    "    for row in f.readlines()[1:]:\n",
    "        line = row.split()\n",
    "        d[line[0]] = line[1:]\n",
    "    cbows_dense.append(d)\n",
    "    \n",
    "#save as separate dictionaries\n",
    "skipgram_vectors_50 = skipgrams[0]\n",
    "skipgram_vectors_150 = skipgrams[1]\n",
    "skipgram_vectors_300 = skipgrams[2]\n",
    "skipgram_dense_vectors_50 = skipgrams_dense[0]\n",
    "skipgram_dense_vectors_150 = skipgrams_dense[1]\n",
    "skipgram_dense_vectors_300 = skipgrams_dense[2]\n",
    "cbow_vectors_50 = cbows[0]\n",
    "cbow_vectors_150 = cbows[1]\n",
    "cbow_vectors_300 = cbows[2]\n",
    "cbow_dense_vectors_50 = cbows_dense[0]\n",
    "cbow_dense_vectors_150 = cbows_dense[1]\n",
    "cbow_dense_vectors_300 = cbows_dense[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implement your own analogy function\n",
    "def analogy_function(dictionary):\n",
    "    scores = []\n",
    "    f = open('analogy_alice.txt' ,'r')\n",
    "    for row in f:\n",
    "        line = row.split()\n",
    "        if all(x in dictionary for x in line):\n",
    "            A = np.array(dictionary[line[0]], dtype=float)\n",
    "            B = np.array(dictionary[line[1]], dtype=float)\n",
    "            C = np.array(dictionary[line[2]], dtype=float)\n",
    "            D = np.array(dictionary[line[3]], dtype=float)        \n",
    "            scores.append(cosine_similarity((A - B + C).reshape(1, -1),D.reshape(1, -1))[0][0])\n",
    "            \n",
    "    return sum(scores)/len(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analogy score for Skipgram model with dim=50: 0.16979255007904148\n",
      "Analogy score for Skipgram model with dim=150: 0.12561215478370516\n",
      "Analogy score for Skipgram model with dim=300: 0.05380436436937661\n",
      "\n",
      "Analogy score for Skipgram + dense model with dim=50: 0.03974015917464052\n",
      "Analogy score for Skipgram + dense model with dim=150: -0.006662141282562735\n",
      "Analogy score for Skipgram + dense model with dim=300: 0.007990631175485945\n",
      "\n",
      "Analogy score for CBOW model with dim=50: 0.12972626636477305\n",
      "Analogy score for CBOW model with dim=150: 0.05903135603289654\n",
      "Analogy score for CBOW model with dim=300: 0.06180547296211673\n",
      "\n",
      "Analogy score for CBOW + dense model with dim=50: -0.07100140598107953\n",
      "Analogy score for CBOW + dense model with dim=150: -0.004404382663867742\n",
      "Analogy score for CBOW + dense model with dim=300: 0.00574616680981604\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#skipgram score\n",
    "skipgram_50_score = analogy_function(skipgram_vectors_50)\n",
    "print(\"Analogy score for Skipgram model with dim=50: \" + str(skipgram_50_score))\n",
    "skipgram_150_score = analogy_function(skipgram_vectors_150)\n",
    "print(\"Analogy score for Skipgram model with dim=150: \" + str(skipgram_150_score))\n",
    "skipgram_300_score = analogy_function(skipgram_vectors_300)\n",
    "print(\"Analogy score for Skipgram model with dim=300: \" + str(skipgram_300_score))\n",
    "print()\n",
    "\n",
    "#skipgram + dense scores\n",
    "skipgram_dense_50_score = analogy_function(skipgram_dense_vectors_50)\n",
    "print(\"Analogy score for Skipgram + dense model with dim=50: \" + str(skipgram_dense_50_score))\n",
    "skipgram_dense_150_score = analogy_function(skipgram_dense_vectors_150)\n",
    "print(\"Analogy score for Skipgram + dense model with dim=150: \" + str(skipgram_dense_150_score))\n",
    "skipgram_dense_300_score = analogy_function(skipgram_dense_vectors_300)\n",
    "print(\"Analogy score for Skipgram + dense model with dim=300: \" + str(skipgram_dense_300_score))\n",
    "print()\n",
    "\n",
    "#cbow score\n",
    "cbow_50_score = analogy_function(cbow_vectors_50)\n",
    "print(\"Analogy score for CBOW model with dim=50: \" + str(cbow_50_score))\n",
    "cbow_150_score = analogy_function(cbow_vectors_150)\n",
    "print(\"Analogy score for CBOW model with dim=150: \" + str(cbow_150_score))\n",
    "cbow_300_score = analogy_function(cbow_vectors_300)\n",
    "print(\"Analogy score for CBOW model with dim=300: \" + str(cbow_300_score))\n",
    "print()\n",
    "\n",
    "#skipgram + dense scores\n",
    "cbow_dense_50_score = analogy_function(cbow_dense_vectors_50)\n",
    "print(\"Analogy score for CBOW + dense model with dim=50: \" + str(cbow_dense_50_score))\n",
    "cbow_dense_150_score = analogy_function(cbow_dense_vectors_150)\n",
    "print(\"Analogy score for CBOW + dense model with dim=150: \" + str(cbow_dense_150_score))\n",
    "cbow_dense_300_score = analogy_function(cbow_dense_vectors_300)\n",
    "print(\"Analogy score for CBOW + dense model with dim=300: \" + str(cbow_dense_300_score))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comparison performance**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the analogy function defined above, we test the analogy calculation of 3 terms, i.e. A-B+C=D, by taking the vector of the expected answer, i.e. D, from the tested model, and then calculating how close is it with the actual vector result of A-B+C, using cosine similarity. The analogies used in the calculation are taken from the file \"analogy_alice.txt\". The analogy score is then defined as the average of cosine similarity score between the expected and the actual vector of the analogy calculations.\n",
    "\n",
    "Looking at the results above, we can conclude that a Skipgram model with 50 dimensions gives the best analogy results. It gives analogy score of 17% - which is not very high - but it is the closest to 1, which means that more analogy calculation results are similar to the expected answer compared to the other Skipgram and CBOW models (with and without dense layers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analogy score for co-occurrance matrix: 0.14999467424101776\n"
     ]
    }
   ],
   "source": [
    "#Comparison to the co-occurrance matrix embeddings    \n",
    "matrix_score = analogy_function(co_occurrence)\n",
    "print(\"Analogy score for co-occurrance matrix: \" + str(matrix_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>50</th>\n",
       "      <th>150</th>\n",
       "      <th>300</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Skipgram</th>\n",
       "      <td>0.169793</td>\n",
       "      <td>0.125612</td>\n",
       "      <td>0.053804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Skipgram + dense</th>\n",
       "      <td>0.039740</td>\n",
       "      <td>-0.006662</td>\n",
       "      <td>0.007991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CBOW</th>\n",
       "      <td>0.129726</td>\n",
       "      <td>0.059031</td>\n",
       "      <td>0.061805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CBOW + dense</th>\n",
       "      <td>-0.071001</td>\n",
       "      <td>-0.004404</td>\n",
       "      <td>0.005746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Co-occurrance matrix</th>\n",
       "      <td>0.149995</td>\n",
       "      <td>0.149995</td>\n",
       "      <td>0.149995</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            50       150       300\n",
       "Skipgram              0.169793  0.125612  0.053804\n",
       "Skipgram + dense      0.039740 -0.006662  0.007991\n",
       "CBOW                  0.129726  0.059031  0.061805\n",
       "CBOW + dense         -0.071001 -0.004404  0.005746\n",
       "Co-occurrance matrix  0.149995  0.149995  0.149995"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x28f4a587eb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Visualization results trained word embeddings\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "scores = pd.DataFrame(columns=[\"50\",\"150\",\"300\"])\n",
    "scores.loc[\"Skipgram\"] = [skipgram_50_score, skipgram_150_score, skipgram_300_score]\n",
    "scores.loc[\"Skipgram + dense\"] = [skipgram_dense_50_score, skipgram_dense_150_score, skipgram_dense_300_score]\n",
    "scores.loc[\"CBOW\"] = [cbow_50_score, cbow_150_score, cbow_300_score]\n",
    "scores.loc[\"CBOW + dense\"] = [cbow_dense_50_score, cbow_dense_150_score, cbow_dense_300_score]\n",
    "scores.loc[\"Co-occurrance matrix\"] = [matrix_score, matrix_score, matrix_score]\n",
    "display(scores)\n",
    "\n",
    "scores = scores.T\n",
    "scores = scores.reindex(scores.index.rename('Size'))\n",
    "ax = scores.plot(figsize=(12,4))\n",
    "ax.set_xticks(range(0, 3))\n",
    "ax.set_xticklabels(scores.index)\n",
    "plt.ylabel('Analogy score')\n",
    "plt.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation results of the visualization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the visualization above, we can see that the analogy score of Skipgram, and Skipgram models with dense layers initially have relatively big difference. However, the more dimensions are included in either types, the closer the analogy score seems to converge. The same thing also happens with CBOW models. In general, it seems like the normal Skipgram and CBOW models perform better than their dense-layered counterparts. It also seems like increasing dimensionality results in the worse performance for all models, except for CBOW with dense layers, which performs the best at 300 dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compare the results of the trained word embeddings with the word-word co-occurrence matrix**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only the "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion of the advantages of CBOW and Skipgram, the advantages of negative sampling and drawbacks of CBOW and Skipgram**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*TODO: after CBOW models are created*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-104-d315c10a206e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mpath_word2vec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"../GoogleNews-vectors-negative300.bin\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mword2vec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_word2vec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\programdata\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[0;32m   1001\u001b[0m         return _load_word2vec_format(\n\u001b[0;32m   1002\u001b[0m             \u001b[0mWord2VecKeyedVectors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfvocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1003\u001b[1;33m             limit=limit, datatype=datatype)\n\u001b[0m\u001b[0;32m   1004\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1005\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_keras_embedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_embeddings\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\gensim\\models\\utils_any2vec.py\u001b[0m in \u001b[0;36m_load_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[0;32m    172\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvector_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m         \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvector_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvector_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 174\u001b[1;33m         \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvectors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvector_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdatatype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    175\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    176\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0madd_word\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#load pretrained word embeddings of word2vec\n",
    "\n",
    "path_word2vec = \"../GoogleNews-vectors-negative300.bin\"\n",
    "\n",
    "word2vec = KeyedVectors.load_word2vec_format(path_word2vec, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load pretraind word embeddings of Glove\n",
    "from gensim.scripts import glove2word2vec\n",
    "\n",
    "path = \"../glove.6B/glove.6B.300d.txt\"\n",
    "\n",
    "#convert GloVe into word2vec format\n",
    "num_vectors, num_dims = glove2word2vec.get_glove_info(path)\n",
    "glove2word2vec.glove2word2vec(path, \"glove_converted.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load glove from saved model\n",
    "glove = KeyedVectors.load_word2vec_format(\"glove_converted.txt\", binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analogy score for glove: 0.3564882017935411\n",
      "Analogy score for word2vec: 0.2910029501014654\n",
      "400000\n"
     ]
    }
   ],
   "source": [
    "glove_score = analogy_function(glove)\n",
    "print(\"Analogy score for glove: \" + str(glove_score))\n",
    "word2vec_score = analogy_function(word2vec)\n",
    "print(\"Analogy score for word2vec: \" + str(word2vec_score))\n",
    "print(num_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Analogy score</th>\n",
       "      <th># vectors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Skipgram 50</th>\n",
       "      <td>0.153818</td>\n",
       "      <td>1182.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Skipgram 150</th>\n",
       "      <td>0.098473</td>\n",
       "      <td>1182.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Skipgram 300</th>\n",
       "      <td>0.077115</td>\n",
       "      <td>1182.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Skipgram 50 + dense</th>\n",
       "      <td>-0.002086</td>\n",
       "      <td>1182.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Skipgram 150 + dense</th>\n",
       "      <td>-0.027959</td>\n",
       "      <td>1182.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Skipgram 300 + dense</th>\n",
       "      <td>-0.000367</td>\n",
       "      <td>1182.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Co-occurrance matrix</th>\n",
       "      <td>-0.003223</td>\n",
       "      <td>1182.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Glove 300</th>\n",
       "      <td>0.356488</td>\n",
       "      <td>400000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word2vec 300</th>\n",
       "      <td>0.291003</td>\n",
       "      <td>3000000.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Analogy score  # vectors\n",
       "Skipgram 50                0.153818     1182.0\n",
       "Skipgram 150               0.098473     1182.0\n",
       "Skipgram 300               0.077115     1182.0\n",
       "Skipgram 50 + dense       -0.002086     1182.0\n",
       "Skipgram 150 + dense      -0.027959     1182.0\n",
       "Skipgram 300 + dense      -0.000367     1182.0\n",
       "Co-occurrance matrix      -0.003223     1182.0\n",
       "Glove 300                  0.356488   400000.0\n",
       "word2vec 300               0.291003  3000000.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x28f446e99e8>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x28f470791d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Visualize the pre-trained word embeddings\n",
    "scores = pd.DataFrame(columns=['Analogy score', '# vectors'])\n",
    "scores.loc[\"Skipgram 50\"] = [skipgram_50_score, len(skipgram_vectors_50.keys())]\n",
    "scores.loc[\"Skipgram 150\"] = [skipgram_150_score, len(skipgram_vectors_150.keys())]\n",
    "scores.loc[\"Skipgram 300\"] = [skipgram_300_score, len(skipgram_vectors_300.keys())]\n",
    "scores.loc[\"Skipgram 50 + dense\"] = [skipgram_dense_50_score, len(skipgram_dense_vectors_50.keys())]\n",
    "scores.loc[\"Skipgram 150 + dense\"] = [skipgram_dense_150_score, len(skipgram_dense_vectors_150.keys())]\n",
    "scores.loc[\"Skipgram 300 + dense\"] = [skipgram_dense_300_score, len(skipgram_dense_vectors_300.keys())]\n",
    "scores.loc[\"CBOW 50\"] = [skipgram_50_score, len(skipgram_vectors_50.keys())]\n",
    "scores.loc[\"CBOW 150\"] = [skipgram_150_score, len(skipgram_vectors_150.keys())]\n",
    "scores.loc[\"CBOW 300\"] = [skipgram_300_score, len(skipgram_vectors_300.keys())]\n",
    "scores.loc[\"CBOW 50 + dense\"] = [skipgram_dense_50_score, len(skipgram_dense_vectors_50.keys())]\n",
    "scores.loc[\"CBOW 150 + dense\"] = [skipgram_dense_150_score, len(skipgram_dense_vectors_150.keys())]\n",
    "scores.loc[\"CBOW 300 + dense\"] = [skipgram_dense_300_score, len(skipgram_dense_vectors_300.keys())]\n",
    "scores.loc[\"Co-occurrance matrix\"] = [matrix_score, len(matrix.columns)]\n",
    "scores.loc[\"Glove \" + str(num_dims)] = [glove_score, len(glove.index2word)]\n",
    "scores.loc[\"word2vec 300\"] = [word2vec_score, len(word2vec.index2word)]\n",
    "display(scores)\n",
    "\n",
    "scores.drop(columns=['# vectors'], inplace=True)\n",
    "scores = scores.T\n",
    "ax = scores.plot(figsize=(12,4), kind='bar')\n",
    "ax.set_xticks(range(0, 3))\n",
    "ax.set_xticklabels(scores.index)\n",
    "plt.ylabel('Analogy score')\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comparison performance with your own trained word embeddings**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown in the graph above, the performance of analogy calculation on Glove pre-trained model, which contains 300 dimensions, far exceeds our own trained models. The same goes with word2vec pre-trained model, which gives the second best analogy score.\n",
    "\n",
    "From the table above the graph, we see that Glove has 400,000 vectors and word2vec has 3,000,000 vectors. In the mean time, our trained models only have 1,182 vectors. This means that the pre-trained models were trained on corpuses that contain more words than \"Alice In the Wonderland\", and therefore have information on more terms than our trained models. Therefore, it is not surprising that the pre-trained models perform better in analogy calculations compared to our own trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
